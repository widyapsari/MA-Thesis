{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22fa7a8-00cb-40e4-a929-efbd9c13524d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe50910-2991-4857-ba6a-391855c6f116",
   "metadata": {},
   "source": [
    "# 1 Interleave space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6e463e-0cfe-4755-a980-2dd3107e0329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interleave(word, char_to_interleave= ' '):\n",
    "    \"\"\"\n",
    "    This function modifies abusive words by interleaving char by space (default) or preferred symbol (\"_\")\n",
    "    \n",
    "    :type: word: abusive word on the df \n",
    "    :char_to_interleave: generate spaces in between the char of word, by default it will generate underscore (\"_\") \n",
    "    \n",
    "    return new_word: obfuscated word\n",
    "    \"\"\"\n",
    "    if len(word) == 1:\n",
    "        return word\n",
    "    new_word = word[0]\n",
    "    for char in word[1:]:\n",
    "        new_word = new_word + char_to_interleave + char\n",
    "    return new_word\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, interleave(l,\" \"))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_1interleave.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_1interleave.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7019336-77f0-4ee4-81f4-1ca41eed6401",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Swap Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0658d053-4781-48b6-ab3c-1690acd8d253",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def swap(word):\n",
    "    \"\"\"\n",
    "    this code will modify the word in df by swapping the first and the second character of word\n",
    "    :type: word:abusive word on the df \n",
    "\n",
    "    \"\"\"\n",
    "    if len(word) < 2:\n",
    "        return word \n",
    "    else:\n",
    "        return word[1] + word[0] + word[2:]\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, swap(l))\n",
    "    return tweet\n",
    "\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_2swapchar.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_2swapchar.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2807a97-b5b1-4ee4-98aa-88f8c3b110e5",
   "metadata": {},
   "source": [
    "# 3. Replace o with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4300aff-a6e3-42ef-bb62-206ebe0af86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 2. create obufscated test dataset using obfuscate_char. o -> 0 ##\n",
    "\n",
    "def obfuscate_char(word, char_to_replace, new_char):\n",
    "    \"\"\"\n",
    "    This function obfuscates the word by replacing first vowel of abusive words with asterisk \n",
    "    \n",
    "    :type: word: abusive word on the df \n",
    "    :char_to_replace: replace the first found vowel in the word\n",
    "    :new_char: the replaced character, in this case first vowel -> *, o -> 0 \n",
    "    \n",
    "    return new_word: obfuscated word\n",
    "    \"\"\"\n",
    "    \n",
    "    first_char_found = False\n",
    "    new_word = \"\"\n",
    "\n",
    "    for char in word:\n",
    "        if not first_char_found and char in char_to_replace:\n",
    "            new_word += new_char\n",
    "            first_char_found = True\n",
    "        else:\n",
    "            new_word += char\n",
    "\n",
    "    return new_word\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, obfuscate_char(l,\"o\",'0'))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_3replace_o.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_3replace_o.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7129f1-d67b-4de7-bf8a-a748f3256b51",
   "metadata": {},
   "source": [
    "# 4. Ommit Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4abc8ed-cc60-491c-9891-06efe605cd37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ommit_char(word, char_to_replace, new_char):\n",
    "    \"\"\"\n",
    "    This function obfuscates the word ommiting all vowels in abusive words\n",
    "     \n",
    "    :type: word: abusive word on the df \n",
    "    :char_to_replace: replace the first found vowel in the word\n",
    "    :new_char: the replaced character, in this case all vowel will be replaced by empty space (\"\") \n",
    "    \n",
    "    return new_word: obfuscated word\n",
    "    \"\"\"\n",
    "        \n",
    "    new_word = \"\"    \n",
    "    for char in word:\n",
    "        if char not in char_to_replace:\n",
    "            new_word += char\n",
    "    return new_word\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, ommit_char(l,\"aiueo\",''))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_4ommit_char.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_4ommit_char.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787972a-c0aa-4dd2-9d38-9e92120da8e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Extra Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b10d54-38de-43df-816b-41b769b3c010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extra_char(word):\n",
    "    \"\"\"\n",
    "    This function will modify the word by adding random ascii character, specified on alphabet\n",
    "    \n",
    "    :type: word: abusive word on df\n",
    "    \"\"\"\n",
    "    import random\n",
    "    random_ascii_character = chr(random.randint(97, 122))\n",
    "    return word + random_ascii_character\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, extra_char(l))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_5extra_char.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_5extra_char.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8087d0f-1887-41b5-9fe7-9694f75093b5",
   "metadata": {},
   "source": [
    "# 6. First vowel to asterisks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e46b90-3689-4497-930c-870c8bb8b965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def obfuscate_char(word, char_to_replace, new_char):\n",
    "    \"\"\"\n",
    "    This function obfuscates the word by replacing first vowel of abusive words with asterisk \n",
    "    \n",
    "    :type: word: abusive word on the df \n",
    "    :char_to_replace: replace the first found vowel in the word\n",
    "    :new_char: the replaced character, in this case first vowel -> *, o -> 0 \n",
    "    \n",
    "    return new_word: obfuscated word\n",
    "    \"\"\"\n",
    "    \n",
    "    first_char_found = False\n",
    "    new_word = \"\"\n",
    "\n",
    "    for char in word:\n",
    "        if not first_char_found and char in char_to_replace:\n",
    "            new_word += new_char\n",
    "            first_char_found = True\n",
    "        else:\n",
    "            new_word += char\n",
    "\n",
    "    return new_word\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, obfuscate_char(l,\"aiueo\",'*'))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_6to_asterisks.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_6to_asterisks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96cbedc7-e2f6-47c0-8e31-608971e5041e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def duplicate_char(word, char_to_duplicate):\n",
    "    \"\"\"\n",
    "    This function will modify the word by adding extra character of first found vowel\n",
    "    \n",
    "    :type: word: abusive word on df\n",
    "    :type: char_to_duplicate ('aiueo')\n",
    "    \n",
    "    return new_word: obfuscated word\n",
    "    \"\"\"\n",
    "    first_char_found = False\n",
    "    new_word = \"\"\n",
    "\n",
    "    for char in word:\n",
    "        if not first_char_found and char in char_to_duplicate:\n",
    "            new_word += char * 5\n",
    "            first_char_found = True\n",
    "        else:\n",
    "            new_word += char\n",
    "\n",
    "    return new_word\n",
    "\n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, duplicate_char(l, \"aiueo\"))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_7duplicate_char.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_7duplicate_char.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b1345-1374-4517-8180-d6d1ca3e27d6",
   "metadata": {},
   "source": [
    "# 8. Random Obfuscation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b9e183a-5801-4ac1-ad77-064121387c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_obfuscation(word):\n",
    "    \"\"\"\n",
    "    This function will modify the word by with all the method above.\n",
    "    \n",
    "    :type: word: abusive word on df\n",
    "    \"\"\"\n",
    "\n",
    "    import random\n",
    "    method = random.randint(0,7)\n",
    "    if method == 0:\n",
    "        return obfuscate_char(word, \"aeiou\", '*')\n",
    "    if method == 1:\n",
    "        return obfuscate_char(word, \"o\", \"0\")\n",
    "    if method == 2:\n",
    "        return ommit_char(word, \"aiueo\", \"\")\n",
    "    if method == 3:\n",
    "        return interleave(word, \" \")\n",
    "    if method == 4:\n",
    "        return interleave(word, \"_\")\n",
    "    if method == 5:\n",
    "        return swap(word)\n",
    "    if method == 6:\n",
    "        return duplicate_char(word, \"aeiou\")\n",
    "    if method == 7:\n",
    "        return extra_char(word)\n",
    "    \n",
    "tweets = pd.read_csv(\"../data/OLID_test.csv\")\n",
    "keyword = pd.read_csv(\"../keyword/keyword_obfuscated.csv\")\n",
    "\n",
    "swearwords = sorted(set(keyword['word'].tolist()))\n",
    "\n",
    "def contains_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def return_sw(tweet, lexicon):\n",
    "    sw = []\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            sw.append(l)\n",
    "    return sw\n",
    "\n",
    "def obfuscate_swearwords(tweet, lexicon):\n",
    "    clean_tweet = set(word_tokenize(tweet))\n",
    "    for l in lexicon:\n",
    "        if l in clean_tweet:\n",
    "            tweet = tweet.replace(l, random_obfuscation(l))\n",
    "    return tweet\n",
    "\n",
    "tweets['found'] = tweets['tweet'].apply(lambda x: contains_swearwords(x, swearwords))\n",
    "tweets['sw_found'] =tweets['tweet'].apply(lambda x: return_sw(x, swearwords))\n",
    "tweets['obfuscated_tweet'] = tweets['tweet'].apply(lambda x: obfuscate_swearwords(x, swearwords))\n",
    "tweets = tweets[['tweet', 'obfuscated_tweet', 'found', 'sw_found', 'label']]\n",
    "tweets.to_csv('../data/olid_obf/OLID_OBF_8random_obf.csv', index=False)\n",
    "\n",
    "#save for BERT dataset, label (0 and 1)\n",
    "tweets['label'] = tweets['label'].replace({'NOT': 0, 'OFF': 1})\n",
    "tweets.to_csv('../data/olid_obf_bert/OLID_OBF_bert_8random_obf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ecb77b-b432-4e23-ab9c-99dc240da94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
